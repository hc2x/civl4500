{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=\"https://colab.research.google.com/github/hc2x/civl4500/blob/main/sovi_exercise/sovi-assessment-tutorial.ipynb\" target=\"_blank\">\n",
    "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/>\n",
    "</a>\n",
    "\n",
    "# Instruction for Google Colab environment\n",
    "- In order to make edits to this notebook, you should press File > \"Save a Copy in Drive\". This will ensure that any edits will be on your local copy, and they will not affect the notebook shared with everyone else.\n",
    "- Click \"Connect\" on the top-right corner. Once you see RAM and Disk, you are ready to run the codes!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "- This Jupyter Notebook demonstrates how to combine socio-economic indicators, flood hazard data, and population data to create a disaster vulnerability assessment at the upazila (sub-district) level in Bangladesh. The steps are as follows:\n",
    "- All data is from the former study ([Lee et al., 2021](https://nhess.copernicus.org/articles/21/1807/2021/)). It is recommended to overview the paper to understand the concept and specifics of vulnerability indicators.\n",
    "- Structure of Exercise:\n",
    "\n",
    "  1. Data Reading and Preprocessing\n",
    "  2. Min-Max Scaling of Indicators\n",
    "  3. Weight Calculation of Indicators (Three Methods: Equal Weights, AHP, PCA)\n",
    "  4. Vulnerability Map Creation\n",
    "  5. Top 10 Most Vulnerable Upazilas (Bar Charts)\n",
    "  6. Flood-Hazard-Based Affected Population Estimation\n",
    "  7. District-Level Population-Weighted Vulnerability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install rasterio\n",
    "! pip install shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the ZIP file and save it as exercise-data.zip\n",
    "!wget -O exercise-data.zip https://github.com/hc2x/civl4500/raw/main/sovi_exercise/exercise-data.zip\n",
    "\n",
    "# Create the subfolder 'data' if it doesn't already exist\n",
    "!mkdir -p data\n",
    "\n",
    "# Unzip the downloaded file into the 'data' folder\n",
    "!unzip exercise-data.zip -d data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.plot import show\n",
    "from shapely.geometry import box\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Optional: make plots inline and set figure size\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install geopandas rasterio matplotlib scikit-learn shapely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Reading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Read the vulnerability indicators data\n",
    "- All data are in `data` folder.\n",
    "- `data_raw.xlsx` contains 26 socio-economic, health, and adaptive capacity indicators for each upazila (`ADM3_PCODE`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data description\n",
    "description_df = pd.read_excel(\"./data/data_table.xlsx\", index_col=0)\n",
    "description_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "table_df = pd.read_excel(\"./data/data_raw.xlsx\")\n",
    "print(\"Table data shape:\")\n",
    "print(table_df.shape)\n",
    "\n",
    "# For demonstration, let's just list them:\n",
    "indicator_cols = [col for col in table_df.columns if col not in ['ADM3_PCODE']]\n",
    "print(\"Table data preview:\")\n",
    "display(table_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_df.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Read the administrative boundary shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "district = gpd.read_file(\"./data/bgd_admbnda_adm2_bbs_20180410.gpkg\")  # 64 districts\n",
    "admin_gdf = gpd.read_file(\"./data/bgd_admbnda_adm3_bbs_20180410.gpkg\")  # 544 upazilas\n",
    "print(\"Admin boundary preview:\")\n",
    "display(admin_gdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the admin boundaries\n",
    "fig, ax = plt.subplots(dpi=125)\n",
    "admin_gdf.plot(ax=ax, linewidth=0.5)\n",
    "ax.set_title('Admin Boundaries')\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Read the flood inundation raster\n",
    "- 1 (Perennial waterbodies), 2 (Flood inundation area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flood Inundation Area (1: Perennial waterbodies, 2: Flood inundation area)\n",
    "flood_raster = rasterio.open('./data/bgd_inun_30m.tif') # 1 (Perennial waterbodies), 2 (Flood inundation area)\n",
    "\n",
    "# Visualize the flood raster\n",
    "fig, ax = plt.subplots(dpi=125)\n",
    "show(flood_raster, ax=ax, title='Flood Inundation Area', cmap='Blues')\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Read the population raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "# Open the population raster file.\n",
    "with rasterio.open('./data/bgd_ppp_2017_30m_decuple.tif') as pop_raster:\n",
    "    pop_data = pop_raster.read(1)\n",
    "\n",
    "# Replace non-positive values with NaN to avoid issues with the log scale.\n",
    "pop_data = np.where(pop_data > 0, pop_data, np.nan)\n",
    "\n",
    "# Visualize the population data with log scale normalization.\n",
    "plt.figure(dpi=125)\n",
    "plt.imshow(pop_data, cmap='Reds', norm=LogNorm())\n",
    "plt.title('Population Per Pixel (Log-scaled)')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.colorbar(label='Population')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Min-Max Scaling of Indicators\n",
    "We'll scale each of the `n` indicators from 0 to 1 using min-max normalization.\n",
    "The formula for min-max scaling:\n",
    "\n",
    "$$X^\\prime = \\frac{X - X_{\\min}}{X_{\\max} - X_{\\min}}$$\n",
    "\n",
    "We'll store these scaled columns in new columns, e.g. col_1_scaled, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll store these scaled columns in new columns, e.g. col_1_scaled, etc.\n",
    "scaler = MinMaxScaler()\n",
    "scaled_values = scaler.fit_transform(table_df[indicator_cols])\n",
    "scaled_df = pd.DataFrame(scaled_values, columns=indicator_cols)\n",
    "scaled_df = pd.concat([table_df[['ADM3_PCODE']], scaled_df], axis=1)\n",
    "\n",
    "print(\"Scaled table data preview:\")\n",
    "display(scaled_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Weight Calculation of Indicators\n",
    "We have three different approaches:\n",
    "1) Equal Weights\n",
    "2) AHP (Analytic Hierarchy Process) Weights\n",
    "3) Custom Weights (survey) Weights\n",
    "4) PCA (Principal Component Analysis)-based Weights\n",
    "\n",
    "After computing each set of weights, we will store them in a single CSV file (weight_data.csv)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Equal Weights\n",
    "- Assign the same weight (1/n) to all indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we have n indicators, each weight is 1/n\n",
    "num_indicators = len(indicator_cols)\n",
    "equal_weights = np.array([1/num_indicators]*num_indicators)\n",
    "equal_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 AHP Weights\n",
    "- We will construct the pairwise comparison matrix from a small survey among students.\n",
    "- For demonstration, let's just assume we have a hypothetical matrix. In practice, you will collect pairwise comparison data from the surveyees and construct the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: If we have 21 indicators, we have a 21x21 matrix. Here, we create a random example.\n",
    "# But typically, you'd fill this with real comparisons. Let's just do a small example for demonstration.\n",
    "\n",
    "# WARNING: The below is a random matrix for demonstration. \n",
    "# Real AHP should follow consistent judgments.\n",
    "#   1) Provide pairwise comparison questions to survey respondents\n",
    "#   2) Collect their responses (e.g., 1-9 scale)\n",
    "#   3) Normalize the responses to create a pairwise comparison matrix\n",
    "#   4) Use the AHP method to derive weights from the pairwise comparison matrix\n",
    "#   5) Ensure the matrix is consistent (e.g., using the Consistency Ratio)\n",
    "#   6) Use the derived weights in your analysis\n",
    "\n",
    "# We'll define a helper function to compute AHP weights from a pairwise matrix.\n",
    "def ahp_weights(pairwise_matrix):\n",
    "    # 1. Compute eigenvalues and eigenvectors\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(pairwise_matrix)\n",
    "    # 2. Principal eigenvector corresponds to the max eigenvalue\n",
    "    max_eig_index = np.argmax(eigenvalues)\n",
    "    principal_eigvec = np.real(eigenvectors[:, max_eig_index])\n",
    "    # 3. Normalize to get weights\n",
    "    normalized_weights = principal_eigvec / principal_eigvec.sum()\n",
    "    return normalized_weights\n",
    "\n",
    "# For demonstration, let's build a random positive matrix with 21x21\n",
    "# and enforce diagonal = 1 (because each indicator is equally compared with itself).\n",
    "rng = np.random.default_rng(42)\n",
    "pairwise_demo = rng.random((num_indicators, num_indicators))*2 + 0.1  # random [0.1,2.1]\n",
    "np.fill_diagonal(pairwise_demo, 1.0)\n",
    "\n",
    "# We need to symmetrize or maintain consistency in a real AHP matrix, but for demonstration:\n",
    "ahp_weight_vector = ahp_weights(pairwise_demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ahp_weight_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Custom weights via Survey\n",
    "- Survey Form: https://forms.gle/ccJTq3RL4UoQdfoc9\n",
    "- Survey Result: https://docs.google.com/spreadsheets/d/13WovhBFI_9q1DR-aJv3PBOf5Hlew5ObxEY3R6ICE2xo/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_average = np.array([4.00, 2.00, 5.00, 5.00, 5.00, 5.00, 4.00, 1.00, 1.00, 1.00, 1.00, 4.00, 4.00, 5.00, 5.00, 4.00, 5.00, 4.00, 5.00, 4.00, 5.00, 5.00, 1.00, 5.00, 1.00, 5.00])\n",
    "custom_weights = custom_average / custom_average.sum()\n",
    "custom_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 PCA-based Weights\n",
    "- We'll use PCA on the scaled socio-economic indicators. \n",
    "- One simple approach is to take the loading of the first principal component as weights (or we could combine multiple PCs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_indicator_data = scaled_df[indicator_cols]\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(scaled_indicator_data)\n",
    "\n",
    "# PCA components_ shape is (n_components, n_features)\n",
    "# We'll take the absolute value of the loadings, or we can keep the sign. \n",
    "# Common approach is to take the absolute values, then normalize to sum = 1\n",
    "pca_component = pca.components_[0]\n",
    "pca_abs = np.abs(pca_component)\n",
    "pca_weights = pca_abs / np.sum(pca_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine weights into one DataFrame and save\n",
    "weight_df = pd.DataFrame({\n",
    "    'Indicator': indicator_cols,\n",
    "    'Equal_Weight': equal_weights,\n",
    "    'AHP_Weight': ahp_weight_vector,\n",
    "    'Custom_Weight': custom_weights,\n",
    "    'PCA_Weight': pca_weights\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "weight_df.to_csv(\"weight_data.csv\", index=False)\n",
    "print(\"Weights have been saved to weight_data.csv\")\n",
    "display(weight_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summation of weights\n",
    "weight_df[['Equal_Weight','AHP_Weight', 'Custom_Weight', 'PCA_Weight']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot of weights\n",
    "plt.figure(dpi=125)\n",
    "weight_df.set_index('Indicator').plot(kind='bar', alpha=0.7)\n",
    "plt.title('Weights for Indicators')\n",
    "plt.xlabel('Indicators')\n",
    "plt.ylabel('Weights')\n",
    "plt.xticks(rotation=60)\n",
    "plt.legend(title='Weight Type')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vulnerability Map Creation\n",
    "\n",
    "We'll create three vulnerability maps: \n",
    "  1) Vulnerability (Equal Weights)\n",
    "  2) Vulnerability (AHP)\n",
    "  3) Vulnerability (Custom Weights)\n",
    "  3) Vulnerability (PCA)\n",
    "\n",
    "For each approach, \n",
    "\n",
    "$$\n",
    "\\text{vulnerability score} = \\sum_{i=1}^{n} \\left( \\text{scaled\\_indicator}_{i} \\cdot \\text{weight}_{i} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_vulnerability_scores(df, indicator_cols, weight_df, weight_name=\"Equal_Weights\"):\n",
    "    \"\"\"\n",
    "    table_df: must have columns [Upazila_ID, col_1_scaled, ..., col_n_scaled]\n",
    "    indicator_cols: original list of indicator column names (not scaled)\n",
    "    weights: 1D array of length n (each indicator's weight)\n",
    "    \"\"\"\n",
    "    scaled_data = df[indicator_cols].values\n",
    "    weights_selected = weight_df[weight_name].values\n",
    "    # # Ensure weights are normalized\n",
    "    # weights_sum = np.sum(weights_selected)\n",
    "    # if weights_sum != 1:\n",
    "    #     weights_selected = weights_selected / weights_sum\n",
    "    # Compute vulnerability score\n",
    "    vul_scores = np.sum(scaled_data * weights_selected.reshape(1, -1), axis=1)\n",
    "    return vul_scores\n",
    "\n",
    "# Compute vulnerability scores using different methods\n",
    "scaled_df['VUL_EQUAL'] = compute_vulnerability_scores(scaled_df, indicator_cols, weight_df, \"Equal_Weight\")\n",
    "scaled_df['VUL_AHP'] = compute_vulnerability_scores(scaled_df, indicator_cols, weight_df, \"AHP_Weight\")\n",
    "scaled_df['VUL_CUSTOM'] = compute_vulnerability_scores(scaled_df, indicator_cols, weight_df, \"Custom_Weight\")\n",
    "scaled_df['VUL_PCA'] = compute_vulnerability_scores(scaled_df, indicator_cols, weight_df, \"PCA_Weight\")\n",
    "\n",
    "print(\"Vulnerability columns added to scaled_df:\")\n",
    "display(scaled_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can join these vulnerability scores to the admin shapefile for mapping.\n",
    "scaled_df['ADM3_PCODE'] = scaled_df['ADM3_PCODE'].astype(str)\n",
    "admin_gdf['ADM3_PCODE'] = admin_gdf['ADM3_PCODE'].astype(str)\n",
    "# Merge the vulnerability scores with the admin shapefile\n",
    "# Note: Ensure the join column names are consistent\n",
    "vul_gdf = admin_gdf.merge(scaled_df, on='ADM3_PCODE', how='left')\n",
    "vul_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with constrained_layout enabled\n",
    "fig, ax = plt.subplots(1, 4, figsize=(12, 6), dpi=125, constrained_layout=True)\n",
    "\n",
    "# Plot maps without individual legends\n",
    "vul_gdf.plot(column='VUL_EQUAL', cmap='bwr', legend=False, vmin=0.2, vmax=0.8, ax=ax[0])\n",
    "ax[0].set_title(\"Equal Weights\")\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "vul_gdf.plot(column='VUL_AHP', cmap='bwr', legend=False, vmin=0.2, vmax=0.8, ax=ax[1])\n",
    "ax[1].set_title(\"AHP Weights\")\n",
    "ax[1].set_xticks([])\n",
    "ax[1].set_yticks([])\n",
    "vul_gdf.plot(column='VUL_CUSTOM', cmap='bwr', legend=False, vmin=0.2, vmax=0.8, ax=ax[2])\n",
    "ax[2].set_title(\"Custom Weights\")\n",
    "ax[2].set_xticks([])\n",
    "ax[2].set_yticks([])\n",
    "vul_gdf.plot(column='VUL_PCA', cmap='bwr', legend=False, vmin=0.2, vmax=0.8, ax=ax[3])\n",
    "ax[3].set_title(\"PCA Weights\")\n",
    "ax[3].set_xticks([])\n",
    "ax[3].set_yticks([])\n",
    "\n",
    "# Plot district boundaries on each axis\n",
    "district.boundary.plot(ax=ax[0], color='grey', linewidth=0.5)\n",
    "district.boundary.plot(ax=ax[1], color='grey', linewidth=0.5)\n",
    "district.boundary.plot(ax=ax[2], color='grey', linewidth=0.5)\n",
    "district.boundary.plot(ax=ax[3], color='grey', linewidth=0.5)\n",
    "\n",
    "# Create a common ScalarMappable for the colorbar based on the colormap and normalization\n",
    "sm = mpl.cm.ScalarMappable(cmap='bwr', norm=mpl.colors.Normalize(vmin=0.2, vmax=0.8))\n",
    "sm._A = []  # empty array for the ScalarMappable\n",
    "\n",
    "# Add a single horizontal colorbar below all subplots\n",
    "cbar = fig.colorbar(sm, ax=ax, orientation='horizontal', fraction=0.05, pad=0.1)\n",
    "cbar.set_label(\"Vulnerability Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vuln",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
